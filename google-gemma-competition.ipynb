{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d530476",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-22T13:08:50.133868Z",
     "iopub.status.busy": "2024-03-22T13:08:50.133530Z",
     "iopub.status.idle": "2024-03-22T13:08:50.945676Z",
     "shell.execute_reply": "2024-03-22T13:08:50.944133Z"
    },
    "papermill": {
     "duration": 0.820594,
     "end_time": "2024-03-22T13:08:50.947677",
     "exception": false,
     "start_time": "2024-03-22T13:08:50.127083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods_detail.csv\n",
      "/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv\n",
      "/kaggle/input/kaggle-winning-solutions-methods/method_category_mapping.csv\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/gemma-2b-it.gguf\n",
      "/kaggle/input/gemma/transformers/2b-it/2/config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/2/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b-it/2/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b-it/2/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b-it/2/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f550e05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:08:50.958472Z",
     "iopub.status.busy": "2024-03-22T13:08:50.958074Z",
     "iopub.status.idle": "2024-03-22T13:09:55.042157Z",
     "shell.execute_reply": "2024-03-22T13:09:55.041059Z"
    },
    "papermill": {
     "duration": 64.092148,
     "end_time": "2024-03-22T13:09:55.044845",
     "exception": false,
     "start_time": "2024-03-22T13:08:50.952697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U huggingface_hub\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4b4302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:09:55.056013Z",
     "iopub.status.busy": "2024-03-22T13:09:55.055701Z",
     "iopub.status.idle": "2024-03-22T13:09:55.990859Z",
     "shell.execute_reply": "2024-03-22T13:09:55.989671Z"
    },
    "papermill": {
     "duration": 0.943035,
     "end_time": "2024-03-22T13:09:55.992973",
     "exception": false,
     "start_time": "2024-03-22T13:09:55.049938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "access_token = UserSecretsClient().get_secret(\"Hugging Face\")\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f480421d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:09:56.004548Z",
     "iopub.status.busy": "2024-03-22T13:09:56.003891Z",
     "iopub.status.idle": "2024-03-22T13:10:40.217078Z",
     "shell.execute_reply": "2024-03-22T13:10:40.215815Z"
    },
    "papermill": {
     "duration": 44.221332,
     "end_time": "2024-03-22T13:10:40.219351",
     "exception": false,
     "start_time": "2024-03-22T13:09:55.998019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaf7afbd7f04113bb47256f0a13dadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 5.66 s, total: 15.8 s\n",
      "Wall time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Check what type of Device enabled (GPU or CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Load the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                        bnb_4bit_use_double_quant=True,\n",
    "                                        bnb_4bit_quant_type=\"nf4\",\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\", quantization_config=quantization_config, low_cpu_mem_usage=True)# Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43968ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:40.231223Z",
     "iopub.status.busy": "2024-03-22T13:10:40.230714Z",
     "iopub.status.idle": "2024-03-22T13:10:57.930934Z",
     "shell.execute_reply": "2024-03-22T13:10:57.929843Z"
    },
    "papermill": {
     "duration": 17.708553,
     "end_time": "2024-03-22T13:10:57.933201",
     "exception": false,
     "start_time": "2024-03-22T13:10:40.224648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 13:10:42.953510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-22 13:10:42.953642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-22 13:10:43.091045: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What is the best thing about Kaggle?\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "**The best thing about Kaggle is its vast and diverse dataset of labeled and unlabeled data, spanning across various domains and industries.** This allows users to explore and analyze problems from diverse perspectives, fostering creativity and problem-solving skills. Additionally, Kaggle offers several features that enhance the data exploration and analysis process, including:\n",
      "\n",
      "* **Interactive data exploration tools:** Users can visualize data through interactive dashboards and explore data using various filtering and sorting options.\n",
      "* **Community features\n",
      "CPU times: user 12.6 s, sys: 1.06 s, total: 13.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = \"What is the best thing about Kaggle?\"\n",
    "# Encode input text to PyTorch tensors\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=100, temperature=0.5)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9adc35ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:57.946055Z",
     "iopub.status.busy": "2024-03-22T13:10:57.945283Z",
     "iopub.status.idle": "2024-03-22T13:10:59.488860Z",
     "shell.execute_reply": "2024-03-22T13:10:59.487956Z"
    },
    "papermill": {
     "duration": 1.552213,
     "end_time": "2024-03-22T13:10:59.491067",
     "exception": false,
     "start_time": "2024-03-22T13:10:57.938854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>place</th>\n",
       "      <th>competition_name</th>\n",
       "      <th>prize</th>\n",
       "      <th>team</th>\n",
       "      <th>kind</th>\n",
       "      <th>metric</th>\n",
       "      <th>year</th>\n",
       "      <th>nm</th>\n",
       "      <th>writeup</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>methods</th>\n",
       "      <th>cleaned_methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n",
       "      <td>2</td>\n",
       "      <td>Google - Isolated Sign Language Recognition</td>\n",
       "      <td>$100,000</td>\n",
       "      <td>1,165</td>\n",
       "      <td>Research</td>\n",
       "      <td>PostProcessorKernelDesc</td>\n",
       "      <td>2023</td>\n",
       "      <td>406306</td>\n",
       "      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n",
       "      <td>2914</td>\n",
       "      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n",
       "      <td>Replace augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n",
       "      <td>2</td>\n",
       "      <td>Google - Isolated Sign Language Recognition</td>\n",
       "      <td>$100,000</td>\n",
       "      <td>1,165</td>\n",
       "      <td>Research</td>\n",
       "      <td>PostProcessorKernelDesc</td>\n",
       "      <td>2023</td>\n",
       "      <td>406306</td>\n",
       "      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n",
       "      <td>2914</td>\n",
       "      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n",
       "      <td>Finger tree rotate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  place  \\\n",
       "0  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n",
       "1  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n",
       "\n",
       "                              competition_name     prize   team      kind  \\\n",
       "0  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n",
       "1  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n",
       "\n",
       "                    metric  year      nm  \\\n",
       "0  PostProcessorKernelDesc  2023  406306   \n",
       "1  PostProcessorKernelDesc  2023  406306   \n",
       "\n",
       "                                             writeup  num_tokens  \\\n",
       "0  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n",
       "1  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n",
       "\n",
       "                                             methods       cleaned_methods  \n",
       "0  ['EfficientNet-B0', 'Data Augmentation', 'Norm...  Replace augmentation  \n",
       "1  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Finger tree rotate  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2649bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.503683Z",
     "iopub.status.busy": "2024-03-22T13:10:59.503384Z",
     "iopub.status.idle": "2024-03-22T13:10:59.510350Z",
     "shell.execute_reply": "2024-03-22T13:10:59.509547Z"
    },
    "papermill": {
     "duration": 0.015509,
     "end_time": "2024-03-22T13:10:59.512335",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.496826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\\n<p>We used only competition data.</p>\\n<h2>1. Data Preprocessing</h2>\\n<h3>1.1 CNN Preprocessing</h3>\\n<ul>\\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\\n<li>During training, we applied various augmentations.</li>\\n<li>We implemented standard normalization.</li>\\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\\n<li>We interpolated the time axis to a size of 160 using \\'nearest\\' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode=\\'nearest\\')</code>.</li>\\n<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\\n</ul>\\n<h3>1.2 Transformer Preprocessing</h3>\\n<ul>\\n<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\\n<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\\n<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\\n<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\\n<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\\n</ul>\\n<p>$$<br>\\n  Motion_{future} = position_{t+1} - position_{t}<br>\\n$$<br>\\n$$<br>\\n  Motion_{history} = position_{t} - position_{t-1}<br>\\n$$</p>\\n<ul>\\n<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\\n<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\\n<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\\n</ul>\\n<h2>2. Augmentation</h2>\\n<h3>2.1 Common Augmentations</h3>\\n<blockquote>\\n  <p>These augmentations are used in both CNN training and transformer training</p>\\n</blockquote>\\n<ol>\\n<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\\n<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\\n<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\\n<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\\n</ol>\\n<h3>2.2 CNN Specific Augmentations</h3>\\n<ul>\\n<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\\n<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\\n<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\\n</ul>\\n<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \\ntime_m = torchaudio.transforms.TimeMasking()       \\n</code></pre>\\n<h3>2.3 Augmented Sample Example</h3>\\n<p>Before augmentation:</p>\\n<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\\n<p>After augmentation:</p>\\n<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\\n<h2>3. Training</h2>\\n<h3>3.1 CNN Training</h3>\\n<ul>\\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\\n<li>Onecycle scheduler with 0.1 warmup.</li>\\n<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\\n<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\\n</ul>\\n<h3>3.2 Transformer Training</h3>\\n<ul>\\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\\n<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\\n<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\\n<li>A 3-layer model was initialized with 4-layer model\\'s first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\\n</ul>\\n<h3>3.3 Hyperparameter Tuning</h3>\\n<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\\n<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\\n<ul>\\n<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\\n<li><p>Learning rate (2e-3 - 3e-3)</p></li>\\n<li><p>Drop out (0.1 - 0.25)</p></li>\\n<li><p>Num of epochs (170-185)</p></li>\\n<li><p>Loss weights powers (0.75 - 2)</p></li>\\n<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\\n<li><p>Label smoothing (0.5 - 0.7)</p></li>\\n</ul>\\n<h2>4. Submissions, Conversion and Ensemble</h2>\\n<ol>\\n<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\\n<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don\\'t know why).</p></li>\\n<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\\n</ol>\\n<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\\n<ol>\\n<li>Efficientnet-B0, fold 0</li>\\n<li>BERT, full data train</li>\\n<li>DeBERTa, full data train</li>\\n</ol>\\n<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\\n<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\\n<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\\nWe spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\\n<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\\n<p>Then we rewrote the depthwise-conv like this:</p>\\n<pre><code>     ():\\n        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\\n         i  (self.kernel_size):\\n             j  (self.kernel_size):\\n                 i ==   j == :\\n                    \\n                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\\n         self.bias   :\\n            out = out + self.bias\\n         out\\n</code></pre>\\n<p>The running time of this is 1.24 ms.</p>\\n<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\\n<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\\n<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\\n<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\\n<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['writeup'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee9e38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.525377Z",
     "iopub.status.busy": "2024-03-22T13:10:59.524714Z",
     "iopub.status.idle": "2024-03-22T13:10:59.529771Z",
     "shell.execute_reply": "2024-03-22T13:10:59.528991Z"
    },
    "papermill": {
     "duration": 0.013382,
     "end_time": "2024-03-22T13:10:59.531523",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.518141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"Competition Name: \" + data['competition_name'][1] + \\\n",
    "    \",\\nYear: \" + data['year'][1].astype(str) + \\\n",
    "    \",\\nPlace: \" + data['place'][1].astype(str) + \\\n",
    "    \",\\nMethods Used: \" + data['methods'][1] + \\\n",
    "    \",\\nSolution: \" + data['writeup'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f8a6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.543951Z",
     "iopub.status.busy": "2024-03-22T13:10:59.543690Z",
     "iopub.status.idle": "2024-03-22T13:10:59.548058Z",
     "shell.execute_reply": "2024-03-22T13:10:59.547221Z"
    },
    "papermill": {
     "duration": 0.013257,
     "end_time": "2024-03-22T13:10:59.550558",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.537301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition Name: Google - Isolated Sign Language Recognition,\n",
      "Year: 2023,\n",
      "Place: 2,\n",
      "Methods Used: ['EfficientNet-B0', 'Data Augmentation', 'Normalization', 'Interpolation', 'BERT', 'DeBERTa', 'Mixup', 'Replace augmentation', 'Time and frequence masking', 'Random affine', 'Random interpolation', 'Flip pose', 'Finger tree rotate', 'Onecycle scheduler', 'Weighted CrossEntropyLoss', 'Hypercolumn', 'Ranger optimizer', 'Transformer', 'Knowledge distillation', 'Optuna', 'Label smoothing'],\n",
      "Solution: <h2>TLDR</h2>\n",
      "<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\n",
      "<p>We used only competition data.</p>\n",
      "<h2>1. Data Preprocessing</h2>\n",
      "<h3>1.1 CNN Preprocessing</h3>\n",
      "<ul>\n",
      "<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\n",
      "<li>During training, we applied various augmentations.</li>\n",
      "<li>We implemented standard normalization.</li>\n",
      "<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\n",
      "<li>We interpolated the time axis to a size of 160 using 'nearest' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest')</code>.</li>\n",
      "<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\n",
      "</ul>\n",
      "<h3>1.2 Transformer Preprocessing</h3>\n",
      "<ul>\n",
      "<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\n",
      "<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\n",
      "<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\n",
      "<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\n",
      "<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\n",
      "</ul>\n",
      "<p>$$<br>\n",
      "  Motion_{future} = position_{t+1} - position_{t}<br>\n",
      "$$<br>\n",
      "$$<br>\n",
      "  Motion_{history} = position_{t} - position_{t-1}<br>\n",
      "$$</p>\n",
      "<ul>\n",
      "<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\n",
      "<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\n",
      "<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\n",
      "</ul>\n",
      "<h2>2. Augmentation</h2>\n",
      "<h3>2.1 Common Augmentations</h3>\n",
      "<blockquote>\n",
      "  <p>These augmentations are used in both CNN training and transformer training</p>\n",
      "</blockquote>\n",
      "<ol>\n",
      "<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\n",
      "<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\n",
      "<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\n",
      "<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\n",
      "</ol>\n",
      "<h3>2.2 CNN Specific Augmentations</h3>\n",
      "<ul>\n",
      "<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\n",
      "<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\n",
      "<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\n",
      "</ul>\n",
      "<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \n",
      "time_m = torchaudio.transforms.TimeMasking()       \n",
      "</code></pre>\n",
      "<h3>2.3 Augmented Sample Example</h3>\n",
      "<p>Before augmentation:</p>\n",
      "<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\n",
      "<p>After augmentation:</p>\n",
      "<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\n",
      "<h2>3. Training</h2>\n",
      "<h3>3.1 CNN Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Onecycle scheduler with 0.1 warmup.</li>\n",
      "<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\n",
      "<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\n",
      "</ul>\n",
      "<h3>3.2 Transformer Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\n",
      "<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\n",
      "<li>A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\n",
      "</ul>\n",
      "<h3>3.3 Hyperparameter Tuning</h3>\n",
      "<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\n",
      "<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\n",
      "<ul>\n",
      "<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\n",
      "<li><p>Learning rate (2e-3 - 3e-3)</p></li>\n",
      "<li><p>Drop out (0.1 - 0.25)</p></li>\n",
      "<li><p>Num of epochs (170-185)</p></li>\n",
      "<li><p>Loss weights powers (0.75 - 2)</p></li>\n",
      "<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\n",
      "<li><p>Label smoothing (0.5 - 0.7)</p></li>\n",
      "</ul>\n",
      "<h2>4. Submissions, Conversion and Ensemble</h2>\n",
      "<ol>\n",
      "<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\n",
      "<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don't know why).</p></li>\n",
      "<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\n",
      "</ol>\n",
      "<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\n",
      "<ol>\n",
      "<li>Efficientnet-B0, fold 0</li>\n",
      "<li>BERT, full data train</li>\n",
      "<li>DeBERTa, full data train</li>\n",
      "</ol>\n",
      "<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\n",
      "<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\n",
      "<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\n",
      "We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\n",
      "<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\n",
      "<p>Then we rewrote the depthwise-conv like this:</p>\n",
      "<pre><code>     ():\n",
      "        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\n",
      "         i  (self.kernel_size):\n",
      "             j  (self.kernel_size):\n",
      "                 i ==   j == :\n",
      "                    \n",
      "                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\n",
      "         self.bias   :\n",
      "            out = out + self.bias\n",
      "         out\n",
      "</code></pre>\n",
      "<p>The running time of this is 1.24 ms.</p>\n",
      "<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\n",
      "<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\n",
      "<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\n",
      "<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\n",
      "<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a29fde9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.563543Z",
     "iopub.status.busy": "2024-03-22T13:10:59.563254Z",
     "iopub.status.idle": "2024-03-22T13:10:59.652826Z",
     "shell.execute_reply": "2024-03-22T13:10:59.651908Z"
    },
    "papermill": {
     "duration": 0.099055,
     "end_time": "2024-03-22T13:10:59.655589",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.556534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition Name: Google - Isolated Sign Language Recognition,\n",
      "Year: 2023,\n",
      "Place: 2,\n",
      "Methods Used: ['EfficientNet-B0', 'Data Augmentation', 'Normalization', 'Interpolation', 'BERT', 'DeBERTa', 'Mixup', 'Replace augmentation', 'Time and frequence masking', 'Random affine', 'Random interpolation', 'Flip pose', 'Finger tree rotate', 'Onecycle scheduler', 'Weighted CrossEntropyLoss', 'Hypercolumn', 'Ranger optimizer', 'Transformer', 'Knowledge distillation', 'Optuna', 'Label smoothing'],\n",
      "Solution: <h2>TLDR</h2>\n",
      "<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\n",
      "<p>We used only competition data.</p>\n",
      "<h2>1. Data Preprocessing</h2>\n",
      "<h3>1.1 CNN Preprocessing</h3>\n",
      "<ul>\n",
      "<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\n",
      "<li>During training, we applied various augmentations.</li>\n",
      "<li>We implemented standard normalization.</li>\n",
      "<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\n",
      "<li>We interpolated the time axis to a size of 160 using 'nearest' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest')</code>.</li>\n",
      "<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\n",
      "</ul>\n",
      "<h3>1.2 Transformer Preprocessing</h3>\n",
      "<ul>\n",
      "<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\n",
      "<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\n",
      "<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\n",
      "<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\n",
      "<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\n",
      "</ul>\n",
      "<p>$$<br>\n",
      "  Motion_{future} = position_{t+1} - position_{t}<br>\n",
      "$$<br>\n",
      "$$<br>\n",
      "  Motion_{history} = position_{t} - position_{t-1}<br>\n",
      "$$</p>\n",
      "<ul>\n",
      "<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\n",
      "<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\n",
      "<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\n",
      "</ul>\n",
      "<h2>2. Augmentation</h2>\n",
      "<h3>2.1 Common Augmentations</h3>\n",
      "<blockquote>\n",
      "  <p>These augmentations are used in both CNN training and transformer training</p>\n",
      "</blockquote>\n",
      "<ol>\n",
      "<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\n",
      "<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\n",
      "<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\n",
      "<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\n",
      "</ol>\n",
      "<h3>2.2 CNN Specific Augmentations</h3>\n",
      "<ul>\n",
      "<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\n",
      "<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\n",
      "<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\n",
      "</ul>\n",
      "<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \n",
      "time_m = torchaudio.transforms.TimeMasking()       \n",
      "</code></pre>\n",
      "<h3>2.3 Augmented Sample Example</h3>\n",
      "<p>Before augmentation:</p>\n",
      "<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\n",
      "<p>After augmentation:</p>\n",
      "<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\n",
      "<h2>3. Training</h2>\n",
      "<h3>3.1 CNN Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Onecycle scheduler with 0.1 warmup.</li>\n",
      "<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\n",
      "<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\n",
      "</ul>\n",
      "<h3>3.2 Transformer Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\n",
      "<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\n",
      "<li>A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\n",
      "</ul>\n",
      "<h3>3.3 Hyperparameter Tuning</h3>\n",
      "<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\n",
      "<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\n",
      "<ul>\n",
      "<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\n",
      "<li><p>Learning rate (2e-3 - 3e-3)</p></li>\n",
      "<li><p>Drop out (0.1 - 0.25)</p></li>\n",
      "<li><p>Num of epochs (170-185)</p></li>\n",
      "<li><p>Loss weights powers (0.75 - 2)</p></li>\n",
      "<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\n",
      "<li><p>Label smoothing (0.5 - 0.7)</p></li>\n",
      "</ul>\n",
      "<h2>4. Submissions, Conversion and Ensemble</h2>\n",
      "<ol>\n",
      "<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\n",
      "<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don't know why).</p></li>\n",
      "<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\n",
      "</ol>\n",
      "<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\n",
      "<ol>\n",
      "<li>Efficientnet-B0, fold 0</li>\n",
      "<li>BERT, full data train</li>\n",
      "<li>DeBERTa, full data train</li>\n",
      "</ol>\n",
      "<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\n",
      "<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\n",
      "<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\n",
      "We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\n",
      "<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\n",
      "<p>Then we rewrote the depthwise-conv like this:</p>\n",
      "<pre><code>     ():\n",
      "        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\n",
      "         i  (self.kernel_size):\n",
      "             j  (self.kernel_size):\n",
      "                 i ==   j == :\n",
      "                    \n",
      "                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\n",
      "         self.bias   :\n",
      "            out = out + self.bias\n",
      "         out\n",
      "</code></pre>\n",
      "<p>The running time of this is 1.24 ms.</p>\n",
      "<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\n",
      "<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\n",
      "<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\n",
      "<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\n",
      "<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>\n"
     ]
    }
   ],
   "source": [
    "data['context'] = (\"Competition Name: \" + data['competition_name'] + \\\n",
    "  \",\\nYear: \" + data['year'].astype(str) + \\\n",
    "   \",\\nPlace: \" + data['place'].astype(str) + \\\n",
    "   \",\\nMethods Used: \" + data['methods'] + \\\n",
    "   \",\\nSolution: \" + data['writeup'])\n",
    "\n",
    "context = data['context'][1]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d74fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.669898Z",
     "iopub.status.busy": "2024-03-22T13:10:59.669618Z",
     "iopub.status.idle": "2024-03-22T13:10:59.675678Z",
     "shell.execute_reply": "2024-03-22T13:10:59.674879Z"
    },
    "papermill": {
     "duration": 0.015267,
     "end_time": "2024-03-22T13:10:59.677555",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.662288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_summary():\n",
    "    prompt_template = f\"\"\"Provide summary of following context in 500 words. \n",
    "\n",
    "    Provide only useful information: \n",
    "    \n",
    "    Context: {context}\"\"\"\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_template},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    outputs = model.generate(input_ids, max_new_tokens=900)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3781e75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.692259Z",
     "iopub.status.busy": "2024-03-22T13:10:59.692006Z",
     "iopub.status.idle": "2024-03-22T13:10:59.696843Z",
     "shell.execute_reply": "2024-03-22T13:10:59.696019Z"
    },
    "papermill": {
     "duration": 0.013757,
     "end_time": "2024-03-22T13:10:59.698670",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.684913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text):\n",
    "    # Find the index of '<start_of_turn>model'\n",
    "    index = text.find('<start_of_turn>model')\n",
    "\n",
    "    # Extract the content after '<start_of_turn>model'\n",
    "    if index != -1:\n",
    "        content_after_model = text[index + len('<start_of_turn>model'):].strip()\n",
    "    else:\n",
    "        return \"Content not found after '<start_of_turn>model'\"\n",
    "    return content_after_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd00c978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:10:59.712297Z",
     "iopub.status.busy": "2024-03-22T13:10:59.711765Z",
     "iopub.status.idle": "2024-03-22T13:11:31.437836Z",
     "shell.execute_reply": "2024-03-22T13:11:31.436843Z"
    },
    "papermill": {
     "duration": 31.741009,
     "end_time": "2024-03-22T13:11:31.445880",
     "exception": false,
     "start_time": "2024-03-22T13:10:59.704871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a summary of the context in 500 words:\n",
      "\n",
      "The context describes the development and training of an EfficientNet-B0 model for isolated sign language recognition. The model is trained on a single fold from 8 randomly split folds, using a combination of common and augmentation augmentations.\n",
      "\n",
      "**Data Preprocessing:**\n",
      "\n",
      "* 80 lip and 21 hand points are extracted from the image.\n",
      "* Various augmentations and normalizations are applied to the extracted features.\n",
      "* Motion features are included, including future and history motion.\n",
      "* Finger tree rotation is used to generate features for the fingers.\n",
      "\n",
      "**Training:**\n",
      "\n",
      "* EfficientNet-B0 is trained on one fold with a random split.\n",
      "* A onecycle scheduler with 0.1 warmup is used.\n",
      "* Weighted CrossEntropyLoss is used to improve the model's performance.\n",
      "* A hypercolumn is implemented for EfficientNet.\n",
      "* BERT and DeBERTa are trained on the full dataset.\n",
      "\n",
      "**Ensemble:**\n",
      "\n",
      "* An ensemble of models is created by aggregating the outputs of the individual models.\n",
      "* The ensemble weights are calculated based on the local fold 0 score.\n",
      "* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n",
      "\n",
      "**Results:**\n",
      "\n",
      "* EfficientNet-B0 achieves a leaderboard score of approximately 0.8.\n",
      "* Transformers improve the score to 0.81.\n",
      "* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "* The model uses a combination of common and augmentation augmentations.\n",
      "* The ensemble weights are calculated based on the local fold 0 score.\n",
      "* EfficientNet-B0 is a highly effective model for isolated sign language recognition.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* The model is trained on a single fold, which may not be representative of the entire dataset.\n",
      "* The model is rewritten in Keras to improve performance.\n",
      "* The running time of the DepthwiseConv2D operation is significantly higher than the other operations.<eos>\n"
     ]
    }
   ],
   "source": [
    "summary = generate_summary()\n",
    "final_summary = extract_content(summary)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f40fdc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T13:11:31.461091Z",
     "iopub.status.busy": "2024-03-22T13:11:31.459817Z",
     "iopub.status.idle": "2024-03-22T13:11:31.466996Z",
     "shell.execute_reply": "2024-03-22T13:11:31.466046Z"
    },
    "papermill": {
     "duration": 0.01668,
     "end_time": "2024-03-22T13:11:31.468960",
     "exception": false,
     "start_time": "2024-03-22T13:11:31.452280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here's a summary of the context in 500 words:\n",
       "\n",
       "The context describes the development and training of an EfficientNet-B0 model for isolated sign language recognition. The model is trained on a single fold from 8 randomly split folds, using a combination of common and augmentation augmentations.\n",
       "\n",
       "**Data Preprocessing:**\n",
       "\n",
       "* 80 lip and 21 hand points are extracted from the image.\n",
       "* Various augmentations and normalizations are applied to the extracted features.\n",
       "* Motion features are included, including future and history motion.\n",
       "* Finger tree rotation is used to generate features for the fingers.\n",
       "\n",
       "**Training:**\n",
       "\n",
       "* EfficientNet-B0 is trained on one fold with a random split.\n",
       "* A onecycle scheduler with 0.1 warmup is used.\n",
       "* Weighted CrossEntropyLoss is used to improve the model's performance.\n",
       "* A hypercolumn is implemented for EfficientNet.\n",
       "* BERT and DeBERTa are trained on the full dataset.\n",
       "\n",
       "**Ensemble:**\n",
       "\n",
       "* An ensemble of models is created by aggregating the outputs of the individual models.\n",
       "* The ensemble weights are calculated based on the local fold 0 score.\n",
       "* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n",
       "\n",
       "**Results:**\n",
       "\n",
       "* EfficientNet-B0 achieves a leaderboard score of approximately 0.8.\n",
       "* Transformers improve the score to 0.81.\n",
       "* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n",
       "\n",
       "**Key Takeaways:**\n",
       "\n",
       "* The model uses a combination of common and augmentation augmentations.\n",
       "* The ensemble weights are calculated based on the local fold 0 score.\n",
       "* EfficientNet-B0 is a highly effective model for isolated sign language recognition.\n",
       "\n",
       "**Additional Notes:**\n",
       "\n",
       "* The model is trained on a single fold, which may not be representative of the entire dataset.\n",
       "* The model is rewritten in Keras to improve performance.\n",
       "* The running time of the DepthwiseConv2D operation is significantly higher than the other operations.<eos>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3475059,
     "sourceId": 6140902,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 167.292824,
   "end_time": "2024-03-22T13:11:34.603528",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-22T13:08:47.310704",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "166048811055476c8b056a122d6a23b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37003c49159648079740d2f9333f019a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3975d38a8c4e4bd9894a0cba03c7d820",
       "placeholder": "",
       "style": "IPY_MODEL_e624037dabdd41b8ae0da680ca85a953",
       "value": " 2/2 [00:36&lt;00:00, 15.04s/it]"
      }
     },
     "3975d38a8c4e4bd9894a0cba03c7d820": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6609ba03a477463d8ecc245d979544b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_166048811055476c8b056a122d6a23b7",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74c6fb207e1b41798909e62d7a6194b6",
       "value": 2.0
      }
     },
     "74c6fb207e1b41798909e62d7a6194b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "82f6f0d5c60b440791c9cfe0c274a8b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9526ba10909f4a9e818da61f87934b51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a434654e37ae43e0b03ac3b7216816e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ccaf7afbd7f04113bb47256f0a13dadc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d90898cb7da2406fb941e01774de0696",
        "IPY_MODEL_6609ba03a477463d8ecc245d979544b1",
        "IPY_MODEL_37003c49159648079740d2f9333f019a"
       ],
       "layout": "IPY_MODEL_a434654e37ae43e0b03ac3b7216816e8"
      }
     },
     "d90898cb7da2406fb941e01774de0696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82f6f0d5c60b440791c9cfe0c274a8b9",
       "placeholder": "",
       "style": "IPY_MODEL_9526ba10909f4a9e818da61f87934b51",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e624037dabdd41b8ae0da680ca85a953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
